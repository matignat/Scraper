{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2408e300",
   "metadata": {},
   "source": [
    "# Analiza: Wykrywanie języka na podstawie częstotliwości słów w tekscie\n",
    "\n",
    "Celem tej pracy jest ocena, czy na podstawie danych dotyczących częstotliwości występowania słów w arytkule na wybranym wiki (u nas bulbapedia) da się rozpoznać język tekstu. Eksperyment polega na porównaniu listy najczęsciej występujących słów na wiki z listą najczęściej występujących słów w jednym z trzech języków. Sprawdzimy dodatkowo jak na poprawność metody wpływa długość arytkułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaeb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Początkowe importy najważniejszych modułów i bibliotek\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from wordfreq import top_n_list, word_frequency\n",
    "from pathlib import Path\n",
    "from scraper_logic import Scraper\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cef5f",
   "metadata": {},
   "source": [
    "Wykorzystamy listy najczęsciej występujących słów z trzech języków: polskiego, angielskiego (język naszej wiki), oraz niemieckiego. Użyjemy do tego biblioteki wordfreq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5765bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zwraca liniową ilość wystąpień słowa 'na jedno słowo' w języku\n",
    "def get_language_words(lang: str, n: int = 1000) -> dict[str, float]:\n",
    "    words = top_n_list(lang, n)\n",
    "    return {w: word_frequency(w, lang) for w in words}\n",
    "\n",
    "    \n",
    "langs = [\"en\", \"pl\", \"de\"]\n",
    "\n",
    "lang_freq = {lang: get_language_words(lang) for lang in langs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ebc9d",
   "metadata": {},
   "source": [
    "Na potrzeby analizy stworzymy nowy uniwersalny scraper, który zadziała dla każdej strony, nie tylko bulbapedii."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1babf4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nasza funkcja scrapująca zwraca obiekt bs, który potem będziemy edytować zależnie od struktury html\n",
    "def get_article(url):\n",
    "    # Tworzymy header, który pozwoli nam się dostać do stron używając headera z httpbin.org/cache\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:147.0) Gecko/20100101 Firefox/147.0'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        source = requests.get(url, headers=headers)\n",
    "    except requests.RequestException:\n",
    "        return None\n",
    "\n",
    "    if source.status_code != 200:\n",
    "        print(f\"Błąd: status_code={source.status_code} dla {url}\")\n",
    "        return None\n",
    "\n",
    "    return BeautifulSoup(source.text, 'html.parser')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03164215",
   "metadata": {},
   "source": [
    "Jako dłuższy artykuł z wybranej wiki (bulbapedii) posłuży strona z opisami umiejętności pokemonów: [https://bulbapedia.bulbagarden.net/wiki/Ability]. Pobierzmy html tej strony używając naszej nowej funkcji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d278b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lo_soup = get_article('https://bulbapedia.bulbagarden.net/wiki/Ability')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ed205",
   "metadata": {},
   "source": [
    "Zrobimy to samo dla krótkiego arytkuły ze strony [https://bulbapedia.bulbagarden.net/wiki/Bulbakaki], który pełen jest nazw własnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a773ee6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh_soup = get_article('https://bulbapedia.bulbagarden.net/wiki/Bulbakaki')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b311292",
   "metadata": {},
   "source": [
    "Robimy to również dla artykułów w każdym z wybranych języków:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "067f9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_soup = get_article('https://wolnelektury.pl/katalog/lektura/witkacy-o-czystej-formie.html')\n",
    "en_soup = get_article('https://minecraft.fandom.com/wiki/Trading')\n",
    "de_soup = get_article('https://wolnelektury.pl/katalog/lektura/heyse-die-schwarze-jakobe.html')\n",
    "\n",
    "# Usuwamy polskie słowa z niemieckiego tekstu (bo pobieramy z polskiej strony)\n",
    "for h2 in de_soup.find_all(\"h2\"):\n",
    "    h2.decompose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00c049",
   "metadata": {},
   "source": [
    "Zdefiniujmy funkcję do liczenia słów zwracającą słownik:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "efc715f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text) -> dict:\n",
    "    word_dict = {}\n",
    "\n",
    "    for word in text:\n",
    "        word_dict[word] = word_dict.get(word, 0) + 1\n",
    "\n",
    "    return word_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5adccb",
   "metadata": {},
   "source": [
    "Teraz musimy w każdej z tych struktur znaleźć właściwą zawartość z artykułem, zamienić ją na tekst i policzyć słowa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d905ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wycinanie właściwej zawartości (find zwraca typ tag)\n",
    "sh_content = sh_soup.find('p')\n",
    "lo_content = lo_soup.find('div', class_='mw-body-content')\n",
    "pl_content = pl_soup.find('div', class_='main-text-body')\n",
    "en_content = en_soup.find('div', id='content')\n",
    "de_content = de_soup.find('div', class_='main-text-body')\n",
    "\n",
    "contents = {\n",
    "    \"sh\": sh_content,\n",
    "    \"lo\": lo_content,\n",
    "    \"pl\": pl_content,\n",
    "    \"en\": en_content,\n",
    "    \"de\": de_content,\n",
    "}\n",
    "\n",
    "dicts = {}\n",
    "\n",
    "for name, c in contents.items():\n",
    "    # Zmieniamy typ na string\n",
    "    text = c.get_text(' ', strip=True)\n",
    "    # Tokenizacja\n",
    "    tokens = re.findall(r'[^\\W\\d_]+', text.lower())\n",
    "    # Tworzymy słowniki z licznikiem wystąptień\n",
    "    res: dict[str, int] = count_words(tokens)\n",
    "    # Sortujemy\n",
    "    sorted_dict = dict(sorted(res.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Wstawiamy do dicts\n",
    "    dicts[name] = sorted_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c203cf",
   "metadata": {},
   "source": [
    "Upewnijmy się, że wszystko działa wypisując pare pierwszych słów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7d219f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 3), ('to', 3), ('bulbagarden', 2), ('oekaki', 2), ('users', 2), ('bulbakaki', 2), ('was', 2), ('known', 1), ('its', 1), ('as', 1), ('an', 1), ('run', 1), ('by', 1), ('which', 1), ('used', 1), ('wacintaki', 1), ('poteto', 1), ('there', 1), ('is', 1), ('a', 1), ('deviantart', 1), ('group', 1), ('for', 1), ('of', 1), ('upload', 1), ('their', 1), ('completed', 1), ('artworks', 1), ('it', 1), ('closed', 1), ('down', 1), ('in', 1)]\n",
      "[('the', 847), ('s', 540), ('pokémon', 500), ('in', 428), ('a', 286), ('ability', 285), ('to', 280), ('its', 247), ('and', 241), ('of', 237), ('it', 168), ('moves', 139), ('abilities', 132), ('from', 121), ('battle', 118), ('when', 114), ('activated', 113), ('used', 110), ('with', 110), ('is', 108), ('by', 93), ('iii', 91), ('that', 87), ('type', 80), ('stat', 70), ('was', 68), ('damage', 63), ('as', 62), ('have', 61), ('hidden', 58), ('or', 56), ('iv', 55), ('attack', 54), ('vii', 53), ('an', 52), ('v', 52), ('ix', 50), ('power', 49), ('viii', 48), ('be', 47), ('if', 47), ('boosts', 45), ('ash', 44), ('for', 43), ('effects', 43), ('has', 43), ('series', 39), ('hit', 39), ('generation', 38), ('will', 38), ('against', 38), ('other', 37), ('hp', 37), ('up', 37), ('their', 36), ('vi', 35), ('are', 34), ('first', 34), ('but', 33), ('changes', 32), ('move', 32), ('same', 32), ('out', 32), ('powers', 32), ('one', 31), ('target', 31), ('on', 30), ('change', 30), ('fire', 29), ('during', 29), ('also', 29), ('speed', 29), ('his', 29), ('while', 28), ('stats', 28), ('into', 27), ('arc', 27), ('can', 27), ('all', 27), ('body', 27), ('enters', 26), ('not', 25), ('activating', 25), ('being', 25), ('status', 25), ('after', 24), ('boosted', 24), ('effect', 23), ('may', 23), ('sp', 23), ('were', 23), ('contact', 23), ('them', 22), ('wild', 22), ('time', 21), ('which', 21), ('most', 21), ('opposing', 21), ('prevents', 21), ('her', 20)]\n",
      "[('w', 278), ('i', 235), ('się', 133), ('nie', 120), ('z', 103), ('na', 101), ('że', 90), ('jest', 85), ('do', 80), ('to', 70), ('a', 65), ('o', 61), ('sztuki', 52), ('od', 52), ('jak', 39), ('może', 37), ('tak', 35), ('dla', 31), ('tylko', 30), ('przypis', 30), ('edytorski', 30), ('co', 29), ('lub', 29), ('tym', 28), ('ale', 28), ('będzie', 28), ('są', 28), ('jako', 28), ('nawet', 26), ('formy', 25), ('tego', 25), ('np', 24), ('ich', 24), ('jego', 23), ('sposób', 21), ('można', 21), ('ten', 20), ('przez', 20), ('jeszcze', 20), ('być', 19), ('które', 18), ('zupełnie', 18), ('ze', 17), ('za', 17), ('możemy', 17), ('czystej', 16), ('zjawisk', 16), ('mamy', 16), ('treści', 15), ('sztuka', 15), ('elementy', 15), ('którą', 15), ('dzieła', 15), ('ponieważ', 15), ('formę', 15), ('jednak', 14), ('innych', 14), ('będziemy', 14), ('mogą', 14), ('teatrze', 14), ('przy', 13), ('u', 13), ('ogóle', 13), ('te', 13), ('tych', 13), ('po', 12), ('których', 12), ('ile', 12), ('sztuce', 12), ('rzeczywistości', 12), ('samo', 12), ('według', 12), ('właśnie', 12), ('jeśli', 12), ('pojęcie', 12), ('następnie', 11), ('czyli', 11), ('bezpośrednio', 11), ('forma', 11), ('ludzie', 11), ('poezji', 11), ('teorii', 10), ('formie', 10), ('nic', 10), ('tej', 10), ('rzeczy', 10), ('mnie', 10), ('danego', 10), ('przedmiotów', 10), ('piękna', 10), ('będą', 10), ('obraz', 10), ('inne', 9), ('tu', 9), ('jej', 9), ('nas', 9), ('zjawiska', 9), ('prawie', 9), ('czy', 9), ('bez', 9)]\n",
      "[('trades', 456), ('emerald', 379), ('the', 264), ('a', 175), ('note', 136), ('of', 127), ('and', 118), ('to', 115), ('villager', 111), ('trade', 98), ('for', 85), ('is', 76), ('in', 74), ('item', 71), ('block', 67), ('level', 61), ('enchanted', 59), ('trading', 58), ('quantity', 56), ('price', 55), ('villagers', 53), ('are', 46), ('with', 45), ('from', 45), ('job', 44), ('book', 43), ('disabled', 42), ('diamond', 41), ('dye', 40), ('site', 38), ('edition', 37), ('until', 36), ('now', 36), ('master', 35), ('wandering', 34), ('an', 33), ('stone', 32), ('novice', 32), ('iron', 32), ('one', 32), ('given', 31), ('items', 30), ('on', 30), ('can', 30), ('multiplier', 30), ('player', 29), ('apprentice', 29), ('journeyman', 29), ('expert', 29), ('economic', 28), ('wanted', 28), ('xp', 28), ('w', 28), ('as', 27), ('default', 27), ('their', 26), ('offer', 26), ('sells', 26), ('trader', 25), ('only', 25), ('by', 25), ('at', 25), ('sell', 25), ('emeralds', 24), ('have', 24), ('be', 24), ('that', 23), ('offers', 22), ('s', 22), ('enchantments', 22), ('or', 21), ('not', 21), ('armor', 21), ('java', 20), ('leather', 20), ('enchantment', 20), ('librarian', 19), ('if', 19), ('it', 19), ('mason', 18), ('this', 18), ('slot', 18), ('table', 18), ('raw', 18), ('any', 17), ('all', 17), ('buy', 17), ('each', 17), ('seeds', 17), ('coral', 16), ('books', 16), ('beta', 16), ('profession', 15), ('they', 15), ('two', 15), ('when', 15), ('lvl', 15), ('bedrock', 14), ('cartographer', 14), ('farmer', 14)]\n",
      "[('und', 581), ('ich', 536), ('sie', 416), ('die', 299), ('der', 227), ('zu', 219), ('nicht', 205), ('in', 183), ('es', 183), ('das', 178), ('mir', 172), ('dass', 156), ('mich', 155), ('den', 138), ('ihr', 126), ('mit', 124), ('war', 123), ('so', 121), ('ein', 120), ('wie', 118), ('aber', 108), ('sich', 103), ('als', 100), ('auf', 100), ('eine', 96), ('du', 95), ('dem', 89), ('er', 89), ('von', 87), ('noch', 86), ('hatte', 86), ('nur', 85), ('an', 83), ('auch', 79), ('wenn', 73), ('habe', 72), ('was', 62), ('sei', 60), ('einen', 54), ('sagte', 52), ('da', 52), ('im', 51), ('ist', 51), ('um', 48), ('meine', 47), ('aus', 46), ('einem', 45), ('dann', 45), ('ganz', 43), ('ihm', 42), ('vor', 41), ('wieder', 41), ('nach', 40), ('immer', 40), ('am', 38), ('sah', 37), ('wir', 36), ('sein', 35), ('frau', 34), ('des', 33), ('mehr', 33), ('meiner', 32), ('nun', 32), ('doch', 32), ('wo', 32), ('für', 31), ('konnte', 30), ('selbst', 30), ('bei', 29), ('durch', 29), ('hat', 29), ('ihre', 29), ('man', 29), ('einer', 28), ('alles', 28), ('meinen', 28), ('über', 27), ('s', 27), ('mein', 27), ('bis', 26), ('gut', 26), ('einmal', 25), ('haben', 25), ('viel', 25), ('mutter', 25), ('uns', 25), ('dir', 25), ('nichts', 24), ('seine', 24), ('wollte', 24), ('kind', 24), ('schon', 23), ('werde', 23), ('bist', 21), ('schwarze', 20), ('denn', 20), ('werden', 20), ('erst', 20), ('unter', 20), ('goldene', 20)]\n"
     ]
    }
   ],
   "source": [
    "for d in dicts:\n",
    "    print(list(d.items())[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aaab22",
   "metadata": {},
   "source": [
    "Pora zdefiniować funckje szacującą język tekstu. Pomysł: liczymy współczynnik **COVERAGE** czyli jaka część wszystkich słów w tekście należy do top-N słów danego języka oraz **COSINE SIMILARITY** czyli porównujemy proporcje tych top-słów w tekście do proporcji w profilu języka (wpisujemy słowa w wektory i patrzymy na kąt pomiędzy nimi mówiący czy idą w tą samą stronę). Wynikiem końcowym będzie coverage * cosine. Zacznijmy od funkcji porównującej proporcje, wzór to:\n",
    "$$\n",
    "\\cos(\\theta)=\\frac{x\\cdot y}{\\|x\\|\\|y\\|}\n",
    "$$\n",
    "Gdzie $0^\\circ$ oznacza całkowite podobieństwo, a $90^\\circ$ jego brak. nie otrzymamy ujemych wartości bo wszystkie el. w wektorach są dodatnie.\n",
    "\n",
    "Nasza norma wektora to:\n",
    "$$\n",
    "\\|x\\| = \\sqrt{\\sum_{i=1}^{n} x_i^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b37fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec_a: dict[str, float], vec_b: dict[str, float]) -> float:\n",
    "    # Ustalamy wspólną liste słów dla obu wektorów\n",
    "    keys = set(vec_a.keys()) | set(vec_b.keys())\n",
    "\n",
    "    # Iloczyn skalarny wektorów i ich normy\n",
    "    dot = 0.0 \n",
    "    norm_a = 0.0\n",
    "    norm_b = 0.0\n",
    "\n",
    "    # Iterujemy po każdym słowie i uzupełniamy dane\n",
    "    for k in keys:\n",
    "        a = vec_a.get(k, 0.0)\n",
    "        b = vec_a.get(k, 0.0)\n",
    "\n",
    "        dot += a * b\n",
    "        norm_a += a * a\n",
    "        norm_b += b * b\n",
    "\n",
    "    # Obliczamy normy\n",
    "    norm_a = math.sqrt(norm_a)\n",
    "    norm_b = math.sqrt(norm_b)\n",
    "\n",
    "    # Dla wektorów pustych zwróć 0 dopasowania\n",
    "    if norm_a == 0.0 or norm_b == 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Zwróc współczynnik cos pomiędzy 0 a 90 stopni\n",
    "    return dot / (norm_a * norm_b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc934c1",
   "metadata": {},
   "source": [
    "Zdefiniujmy funkcję, która na bazie coverage i cosine similarity obliczy współczynnik dopasowania języka, zwracającą wynik 0..1 gdzie 0 to brak dopasowania a 1 to pewne dopasowanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f53300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_confidence_score(word_counts: dict[str, int],\n",
    "                          language_words_with_frequency: dict[str, float]) -> float:\n",
    "    if not word_counts or not language_words_with_frequency:\n",
    "        return 0.0\n",
    "    \n",
    "    total_words = sum(word_counts.values())\n",
    "    if total_words <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Budujemy wektory na podstawie słowników częstotliwości\n",
    "    lang_weights: dict[str, float] = {}\n",
    "\n",
    "    for w, f in language_words_with_frequency.items():\n",
    "        weight = float(f)\n",
    "        if weight > 0:\n",
    "            lang_weights[w] = weight\n",
    "    \n",
    "    if not lang_weights:\n",
    "        return 0.0\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
