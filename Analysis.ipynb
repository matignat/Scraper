{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2408e300",
   "metadata": {},
   "source": [
    "# Analiza: Wykrywanie języka na podstawie częstotliwości słów w tekscie\n",
    "\n",
    "Celem tej pracy jest ocena, czy na podstawie danych dotyczących częstotliwości występowania słów w arytkule na wybranym wiki (u nas bulbapedia) da się rozpoznać język tekstu. Eksperyment polega na porównaniu listy najczęsciej występujących słów na wiki z listą najczęściej występujących słów w jednym z trzech języków. Sprawdzimy dodatkowo jak na poprawność metody wpływa długość arytkułu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aaeb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Początkowe importy najważniejszych modułów i bibliotek\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from wordfreq import top_n_list, zipf_frequency\n",
    "from pathlib import Path\n",
    "from scraper_logic import Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12cef5f",
   "metadata": {},
   "source": [
    "Wykorzystamy listy najczęsciej występujących słów z trzech języków: polskiego, angielskiego (język naszej wiki), oraz niemieckiego. Użyjemy do tego biblioteki wordfreq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5765bec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_language_words(lang, n=1000):\n",
    "    words = top_n_list(lang, n)\n",
    "    return {\n",
    "        w : zipf_frequency(w, lang)\n",
    "        for w in words \n",
    "    }\n",
    "    \n",
    "en_dict = get_language_words('en')\n",
    "pl_dict = get_language_words('pl')\n",
    "de_dict = get_language_words('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b8aab",
   "metadata": {},
   "source": [
    "Do pobrania słów z arytkułów z wiki użyjemy pliku tworzonego wywołaniem funkcji scrapera do_count_words(), która aktualizuje plik o słownik [słowa : liczba wystąpień] dla danej frazy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2823db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_file(): \n",
    "    with open('./word-counts.json', 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "    \n",
    "def get_words(phrase, base_url='https://bulbapedia.bulbagarden.net/wiki/'):\n",
    "    # Tworzymy instancje scrapera dla danej frazy.\n",
    "    scraper = Scraper(phrase, base_url)\n",
    "\n",
    "    # Usuwamy zawartość pliku word-count.json\n",
    "    try:\n",
    "        Path('./word-counts.json').unlink()\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    \n",
    "    # Liczymy słowa dla danego artykułu\n",
    "    scraper.do_count_words()\n",
    "\n",
    "    # Zwracamy otrzymany słownik\n",
    "    return read_word_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03164215",
   "metadata": {},
   "source": [
    "Przygotujmy i zapiszmy do zmiennej dane z długiego artykułu. Tworzymy dedykowany plik word-analysis.json, używamy scrapera do zapełnienia go słowami z danego artykułu i zapisujemy z użyciem powyższej funkcji. Jako długi arytykuł użyjemy opisu regionu Paldea z bulbapedii [https://bulbapedia.bulbagarden.net/wiki/Paldea]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d278b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_article = get_words('Paldea')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1ed205",
   "metadata": {},
   "source": [
    "Zrobimy to samo dla krótkiego arytkuły ze strony [https://bulbapedia.bulbagarden.net/wiki/Bulbakaki], który pełen jest nazw własnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd49c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "short_article = get_words('Bulbakaki')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b311292",
   "metadata": {},
   "source": [
    "Musimy zrobić to samo dla artykułów w każdym z wybranych języków do czego również użyjemy scrapera ale z podmienionym base_url, skorzystamy ze strony wikipedii     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067f9535",
   "metadata": {},
   "outputs": [
    {
     "ename": "ArticleNotFoundError",
     "evalue": "Could not find article content area.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mArticleNotFoundError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m pl_article = \u001b[43mget_words\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mInformatyka\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhttps://pl.wikipedia.org/wiki/\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(pl)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_words\u001b[39m\u001b[34m(phrase, base_url)\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Liczymy słowa dla danego artykułu\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mscraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdo_count_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Zwracamy otrzymany słownik\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m read_word_file()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mati\\Desktop\\Scraper\\scraper_logic.py:105\u001b[39m, in \u001b[36mScraper.do_count_words\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdo_count_words\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_source\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.get_text(strip=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    106\u001b[39m     \u001b[38;5;66;03m# Use regex to cut only words and make Upper case = Lower case\u001b[39;00m\n\u001b[32m    107\u001b[39m     words = re.findall(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+\u001b[39m\u001b[33m'\u001b[39m, text.lower())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Mati\\Desktop\\Scraper\\scraper_logic.py:56\u001b[39m, in \u001b[36mScraper.get_source\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     53\u001b[39m content = soup.find(\u001b[33m'\u001b[39m\u001b[33mdiv\u001b[39m\u001b[33m'\u001b[39m, class_=\u001b[33m'\u001b[39m\u001b[33mmw-parser-output\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m content \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.ArticleNotFoundError(\u001b[33m'\u001b[39m\u001b[33mCould not find article content area.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m content\n",
      "\u001b[31mArticleNotFoundError\u001b[39m: Could not find article content area."
     ]
    }
   ],
   "source": [
    "    pl_article = get_words('Informatyka', 'https://pl.wikipedia.org/wiki/')\n",
    "print(pl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
